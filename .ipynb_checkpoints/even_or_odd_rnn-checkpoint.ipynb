{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "We are going to build a recurrent neural network that learns to classify if the sentence is solely composed of even or odd digits. Although deep learning is unncessary for the task, this simple practice helps to understand the basics of RNN. So, let's build it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shatapy/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h_size = 128\n",
    "embedding_dim = 64\n",
    "n_class = 2\n",
    "time_steps = 6\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to_word_map = {1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six', 7: 'seven', 8: 'eight', 9: 'nine'}\n",
    "num_to_word_map[0] = 'PAD'  # zero padding for the sentence shorter than time_steps 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "even_sent = []  # list of sentences composed of even digits\n",
    "odd_sent = []\n",
    "seq_lens = []  # list of lengths of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate odd and even sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put a zero padding to the sentences shorter than `time_steps 6` to feed data of consistent shape to basic rnn network. However, this creates an unnecessary noise that the network has to learn additionally. So, make sure tolet `tf.nn.dynamic_rnn()` know where the sentence ends so that it can only consider the necessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.randint(3, 7)  # length of 3 ~ 6\n",
    "    seq_lens.append(rand_seq_len)\n",
    "    \n",
    "    odd_nums = np.random.choice(range(1, 10, 2), rand_seq_len)  # 3, 5, 1, 3\n",
    "    even_nums = np.random.choice(range(2, 10, 2), rand_seq_len)\n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        odd_nums = np.append(odd_nums, [0]*(6-rand_seq_len))\n",
    "        even_nums = np.append(even_nums, [0]*(6-rand_seq_len))\n",
    "    \n",
    "    odd_sent.append(\" \".join([num_to_word_map[i] for i in odd_nums]))\n",
    "    even_sent.append(\" \".join([num_to_word_map[i] for i in even_nums]))\n",
    "    \n",
    "data = even_sent + odd_sent  # remember this order. even first. odd later!\n",
    "seq_lens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two four six eight PAD PAD',\n",
       " 'two six two six eight PAD',\n",
       " 'two eight six PAD PAD PAD',\n",
       " 'eight eight two PAD PAD PAD',\n",
       " 'four eight eight four two four',\n",
       " 'two eight eight PAD PAD PAD',\n",
       " 'four six six PAD PAD PAD',\n",
       " 'eight four two eight PAD PAD',\n",
       " 'six two four six PAD PAD',\n",
       " 'two six two two four two']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sent[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the \"Token Ids\" to each digits\n",
    "The ids don't have a special meaning in itself. They are just indices randomly put to each digits they meet first. <strong>This means that the id 1 doesn't necessary imply that its value is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "idx = 0\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "index2word = {index:word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'two',\n",
       " 1: 'four',\n",
       " 2: 'six',\n",
       " 3: 'eight',\n",
       " 4: 'PAD',\n",
       " 5: 'one',\n",
       " 6: 'three',\n",
       " 7: 'five',\n",
       " 8: 'seven',\n",
       " 9: 'nine'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make one-hot labels and split data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000 # [even*10000, odd*10000]. Remember the order of data above? Even first. Odd later\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot = [0, 0]\n",
    "    one_hot[label] = 1  # if label == 1, then one_hot=[0, 1], which is even.\n",
    "    labels[i] = one_hot\n",
    "    \n",
    "indices = list(range(len(data)))\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "data = np.array(data)[indices]\n",
    "labels = np.array(labels)[indices]\n",
    "seq_lens = np.array(seq_lens)[indices]\n",
    "\n",
    "num_test = len(data) // 10  # train: test = 9: 1\n",
    "\n",
    "train_x = data[num_test:]\n",
    "train_y = labels[num_test:]\n",
    "train_seq_lens = seq_lens[num_test:]\n",
    "\n",
    "test_x = data[:num_test]\n",
    "test_y = labels[:num_test]\n",
    "test_seq_lens = seq_lens[:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent_batch(batch_size, data_x, data_y, data_seq_lens):\n",
    "    temp_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(temp_indices)\n",
    "    batch_indices = temp_indices[:batch_size]\n",
    "    x = [[word2index[word] for word in data_x[i].split()] for i in batch_indices]\n",
    "    y = [data_y[i] for i in batch_indices]\n",
    "    seqlens = [data_seq_lens[i] for i in batch_indices]\n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [batch_size, time_steps], name='X')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_class], name='Y')\n",
    "\n",
    "_seqlens = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_dim], -1., 1., name='embedding'))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('LSTM'):\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(h_size, forget_bias=1.)\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, embed, sequence_length=_seqlens, dtype=tf.float32)\n",
    "    W = tf.get_variable('W_linear', [h_size, n_class], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable('b_linear', [n_class], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    logits = tf.matmul(states[1], W) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_preds = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "acc_op = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.5763219  Acc:  92.1875\n",
      "Epoch:  100  Loss:  0.0003720575  Acc:  100.0\n",
      "Epoch:  200  Loss:  0.00018293489  Acc:  100.0\n",
      "Epoch:  300  Loss:  8.4165615e-05  Acc:  100.0\n",
      "Epoch:  400  Loss:  5.528539e-05  Acc:  100.0\n",
      "Epoch:  500  Loss:  4.125999e-05  Acc:  100.0\n",
      "Epoch:  600  Loss:  2.6504116e-05  Acc:  100.0\n",
      "Epoch:  700  Loss:  2.184145e-05  Acc:  100.0\n",
      "Epoch:  800  Loss:  1.8867082e-05  Acc:  100.0\n",
      "Epoch:  900  Loss:  1.4938028e-05  Acc:  100.0\n",
      "Pred:  [0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0\n",
      " 0 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0]  Acc:  100.0\n",
      "Pred:  [0 1 0 1 0 0 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0\n",
      " 0 0 1 0 0 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 0 0 0 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 1]  Acc:  100.0\n",
      "Pred:  [0 0 0 1 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0\n",
      " 1 0 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 1 1 0 0\n",
      " 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 0 0 1 1 0 0 1 1 0 0]  Acc:  100.0\n",
      "Pred:  [0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1\n",
      " 0 0 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
      " 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1\n",
      " 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0]  Acc:  100.0\n",
      "Pred:  [0 0 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1\n",
      " 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 0\n",
      " 0 1 0 0 1 0 1 0 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 1 0\n",
      " 0 0 0 0 0 1 1 0 1 0 1 1 1 1 0 0 1]  Acc:  100.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sent_batch(batch_size, train_x, train_y, train_seq_lens)\n",
    "        sess.run(train_op, feed_dict={X: x_batch, Y: y_batch, _seqlens: seqlen_batch})\n",
    "        \n",
    "        if not epoch % 100:\n",
    "            _loss, acc = sess.run([loss, acc_op], feed_dict={X: x_batch, Y: y_batch, _seqlens: seqlen_batch})\n",
    "            print('Epoch: ', epoch, ' Loss: ', _loss, ' Acc: ', acc)\n",
    "    \n",
    "    for i in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sent_batch(batch_size, test_x, test_y, test_seq_lens)\n",
    "        pred, test_acc = sess.run([tf.argmax(logits, 1), acc_op], feed_dict={X: x_test, Y: y_test, _seqlens: seqlen_test})\n",
    "        print('Acc: ', acc)\n",
    "    \n",
    "    output_ex, state_ex = sess.run([outputs, states], feed_dict={X: x_test, Y: y_test, _seqlens: seqlen_test})\n",
    "    print('output: ', output_ex)\n",
    "    print('state: ', )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
