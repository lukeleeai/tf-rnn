{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "We are going to build a recurrent neural network with two LSTM layers. Please check out even_or_odd_rnn code first.  \n",
    "TensorFlow provides tf.contrib.rnn.MultiRNNCell that runs through the LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shatapy/anaconda/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "h_size = 128\n",
    "embedding_dim = 64\n",
    "n_class = 2\n",
    "time_steps = 6\n",
    "element_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_to_word_map = {1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six', 7: 'seven', 8: 'eight', 9: 'nine'}\n",
    "num_to_word_map[0] = 'PAD'  # zero padding for the sentence shorter than time_steps 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "even_sent = []  # list of sentences composed of even digits\n",
    "odd_sent = []\n",
    "seq_lens = []  # list of lengths of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate odd and even sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put a zero padding to the sentences shorter than `time_steps 6` to feed data of consistent shape to basic rnn network. However, this creates an unnecessary noise that the network has to learn additionally. So, make sure tolet `tf.nn.dynamic_rnn()` know where the sentence ends so that it can only consider the necessary features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.randint(3, 7)  # length of 3 ~ 6\n",
    "    seq_lens.append(rand_seq_len)\n",
    "    \n",
    "    odd_nums = np.random.choice(range(1, 10, 2), rand_seq_len)  # 3, 5, 1, 3\n",
    "    even_nums = np.random.choice(range(2, 10, 2), rand_seq_len)\n",
    "    \n",
    "    if rand_seq_len < 6:\n",
    "        odd_nums = np.append(odd_nums, [0]*(6-rand_seq_len))\n",
    "        even_nums = np.append(even_nums, [0]*(6-rand_seq_len))\n",
    "    \n",
    "    odd_sent.append(\" \".join([num_to_word_map[i] for i in odd_nums]))\n",
    "    even_sent.append(\" \".join([num_to_word_map[i] for i in even_nums]))\n",
    "    \n",
    "data = even_sent + odd_sent  # remember this order. even first. odd later!\n",
    "seq_lens *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eight two two four PAD PAD',\n",
       " 'eight eight eight PAD PAD PAD',\n",
       " 'eight two four two eight four',\n",
       " 'two eight four two eight PAD',\n",
       " 'six four eight two PAD PAD',\n",
       " 'eight eight two six eight PAD',\n",
       " 'eight four four eight two PAD',\n",
       " 'four eight two PAD PAD PAD',\n",
       " 'eight six four PAD PAD PAD',\n",
       " 'four two two two eight PAD']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_sent[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map the \"Token Ids\" to each digits\n",
    "The ids don't have a special meaning in itself. They are just indices randomly put to each digits they meet first. <strong>This means that the id 1 doesn't necessary imply that its value is one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "idx = 0\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index:\n",
    "            word2index[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "index2word = {index:word for word, index in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'eight',\n",
       " 1: 'two',\n",
       " 2: 'four',\n",
       " 3: 'PAD',\n",
       " 4: 'six',\n",
       " 5: 'five',\n",
       " 6: 'three',\n",
       " 7: 'one',\n",
       " 8: 'nine',\n",
       " 9: 'seven'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make one-hot labels and split data into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = [1]*10000 + [0]*10000 # [even*10000, odd*10000]. Remember the order of data above? Even first. Odd later\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot = [0, 0]\n",
    "    one_hot[label] = 1  # if label == 1, then one_hot=[0, 1], which is even.\n",
    "    labels[i] = one_hot\n",
    "    \n",
    "indices = list(range(len(data)))\n",
    "np.random.shuffle(indices)\n",
    "    \n",
    "data = np.array(data)[indices]\n",
    "labels = np.array(labels)[indices]\n",
    "seq_lens = np.array(seq_lens)[indices]\n",
    "\n",
    "num_test = len(data) // 10  # train: test = 9: 1\n",
    "\n",
    "train_x = data[num_test:]\n",
    "train_y = labels[num_test:]\n",
    "train_seq_lens = seq_lens[num_test:]\n",
    "\n",
    "test_x = data[:num_test]\n",
    "test_y = labels[:num_test]\n",
    "test_seq_lens = seq_lens[:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sent_batch(batch_size, data_x, data_y, data_seq_lens):\n",
    "    temp_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(temp_indices)\n",
    "    batch_indices = temp_indices[:batch_size]\n",
    "    x = [[word2index[word] for word in data_x[i].split()] for i in batch_indices]\n",
    "    y = [data_y[i] for i in batch_indices]\n",
    "    seqlens = [data_seq_lens[i] for i in batch_indices]\n",
    "    return x, y, seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [batch_size, time_steps], name='X')\n",
    "Y = tf.placeholder(tf.float32, [batch_size, n_class], name='Y')\n",
    "\n",
    "_seqlens = tf.placeholder(tf.int32, [batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed the words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('embeddings'):\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_dim], -1., 1., name='embedding'))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_LSTM_layers = 2\n",
    "with tf.variable_scope('LSTM'):\n",
    "    lstm_cells = [tf.contrib.rnn.BasicLSTMCell(h_size, forget_bias=1.) for _ in range(n_LSTM_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(lstm_cells, state_is_tuple=True)\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, embed, sequence_length=_seqlens, dtype=tf.float32)\n",
    "    W = tf.get_variable('W_linear', [h_size, n_class], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b = tf.get_variable('b_linear', [n_class], initializer=tf.zeros_initializer())\n",
    "    \n",
    "    logits = tf.matmul(states[n_LSTM_layers-1][1], W) + b\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_preds = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "acc_op = tf.reduce_mean(tf.cast(correct_preds, tf.float32)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.6586384  Acc:  92.1875\n",
      "Epoch:  100  Loss:  5.045624e-05  Acc:  100.0\n",
      "Epoch:  200  Loss:  2.7901591e-05  Acc:  100.0\n",
      "Epoch:  300  Loss:  2.6127353e-05  Acc:  100.0\n",
      "Epoch:  400  Loss:  2.0469604e-05  Acc:  100.0\n",
      "Epoch:  500  Loss:  1.0959508e-05  Acc:  100.0\n",
      "Epoch:  600  Loss:  8.933061e-06  Acc:  100.0\n",
      "Epoch:  700  Loss:  7.7735995e-06  Acc:  100.0\n",
      "Epoch:  800  Loss:  5.2032083e-06  Acc:  100.0\n",
      "Epoch:  900  Loss:  3.9003335e-06  Acc:  100.0\n",
      "Acc:  100.0\n",
      "Acc:  100.0\n",
      "Acc:  100.0\n",
      "Acc:  100.0\n",
      "Acc:  100.0\n",
      "output:  [[[ 7.9929784e-02  6.8176682e-03  5.7104353e-02 ...  3.5779770e-02\n",
      "   -3.0442338e-02 -9.0788461e-02]\n",
      "  [ 3.1010464e-01 -2.5449570e-03  2.3223484e-01 ...  2.3181106e-01\n",
      "   -8.2969993e-02 -3.5609621e-01]\n",
      "  [ 6.6815746e-01 -2.2718480e-03  5.2944988e-01 ...  6.4440733e-01\n",
      "   -7.6559059e-02 -6.7584026e-01]\n",
      "  [ 8.8369554e-01 -4.3445607e-04  7.3655599e-01 ...  8.9715606e-01\n",
      "   -5.2254144e-02 -8.5723937e-01]\n",
      "  [ 9.4818819e-01 -7.6858414e-05  8.3095914e-01 ...  9.6909106e-01\n",
      "   -2.5660897e-02 -9.2525870e-01]\n",
      "  [ 9.6539998e-01 -3.5430694e-05  8.5538816e-01 ...  9.8346752e-01\n",
      "   -1.7011721e-02 -9.4269300e-01]]\n",
      "\n",
      " [[-4.1413650e-02  5.4746278e-02 -1.4477940e-02 ... -1.7638804e-02\n",
      "    6.0662050e-02  1.8766550e-02]\n",
      "  [-1.5383495e-01  3.0322003e-01 -8.6237356e-02 ... -4.5209531e-02\n",
      "    2.8806582e-01  1.5426315e-01]\n",
      "  [-4.0499485e-01  7.6326591e-01 -1.3828194e-01 ... -4.3017004e-02\n",
      "    7.0431948e-01  3.3469221e-01]\n",
      "  [-7.0258391e-01  9.5915449e-01 -1.2650242e-01 ... -1.6766228e-02\n",
      "    9.3212622e-01  4.3920794e-01]\n",
      "  [-8.2547969e-01  9.9286002e-01 -1.0398240e-01 ... -6.1166054e-03\n",
      "    9.8365462e-01  4.5340624e-01]\n",
      "  [-8.6750269e-01  9.9781215e-01 -8.9685306e-02 ... -3.4708236e-03\n",
      "    9.9203527e-01  4.6636921e-01]]\n",
      "\n",
      " [[ 6.8029754e-02  1.8010700e-02  3.7827775e-02 ...  3.4961447e-02\n",
      "   -7.9585519e-03 -8.5625120e-02]\n",
      "  [ 2.8192183e-01  1.6234767e-02  1.4378221e-01 ...  1.9346730e-01\n",
      "   -6.6833682e-02 -3.1330636e-01]\n",
      "  [ 6.3623750e-01  7.6948194e-04  4.3978614e-01 ...  5.8722854e-01\n",
      "   -8.7374307e-02 -6.4773613e-01]\n",
      "  [ 8.7061399e-01 -2.3093072e-04  7.0438468e-01 ...  8.8087684e-01\n",
      "   -5.4428786e-02 -8.5403562e-01]\n",
      "  [ 9.4849581e-01 -6.5174994e-05  8.1386834e-01 ...  9.6506935e-01\n",
      "   -2.6410110e-02 -9.2113996e-01]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-4.1413650e-02  5.4746278e-02 -1.4477940e-02 ... -1.7638804e-02\n",
      "    6.0662050e-02  1.8766550e-02]\n",
      "  [-1.8153095e-01  2.9532602e-01 -6.8485834e-02 ... -4.5423474e-02\n",
      "    3.0308497e-01  1.4199463e-01]\n",
      "  [-4.2871478e-01  7.4985892e-01 -1.2770745e-01 ... -4.6081688e-02\n",
      "    6.9801098e-01  3.1987444e-01]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 7.8222580e-02  2.5567718e-02  4.3859016e-02 ...  3.6686976e-02\n",
      "   -2.4413645e-02 -7.6621667e-02]\n",
      "  [ 3.1152570e-01  1.7677467e-02  2.0029749e-01 ...  2.1066947e-01\n",
      "   -7.1148559e-02 -3.3853319e-01]\n",
      "  [ 6.7144686e-01  1.2973526e-03  5.0793266e-01 ...  6.2766057e-01\n",
      "   -8.4040403e-02 -6.8067777e-01]\n",
      "  [ 8.8956529e-01 -8.2237355e-05  7.4078351e-01 ...  8.9897323e-01\n",
      "   -4.7813017e-02 -8.7383670e-01]\n",
      "  [ 9.5200360e-01 -4.6329034e-05  8.2772690e-01 ...  9.6868730e-01\n",
      "   -2.4047341e-02 -9.3045902e-01]\n",
      "  [ 9.6158350e-01 -3.2193988e-05  8.5678256e-01 ...  9.8286706e-01\n",
      "   -1.6690508e-02 -9.4167775e-01]]\n",
      "\n",
      " [[ 7.9929784e-02  6.8176682e-03  5.7104353e-02 ...  3.5779770e-02\n",
      "   -3.0442338e-02 -9.0788461e-02]\n",
      "  [ 3.1010464e-01 -2.5449570e-03  2.3223484e-01 ...  2.3181106e-01\n",
      "   -8.2969993e-02 -3.5609621e-01]\n",
      "  [ 6.7495316e-01 -2.2095246e-03  5.3561789e-01 ...  6.4828998e-01\n",
      "   -8.2973965e-02 -6.8894130e-01]\n",
      "  [ 8.9246649e-01 -3.3381814e-04  7.5175279e-01 ...  9.0320230e-01\n",
      "   -4.8433334e-02 -8.7531930e-01]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]]\n",
      "state:  (LSTMStateTuple(c=array([[ 1.5814673e+00,  1.6077766e+00,  1.0646734e+00, ...,\n",
      "         2.7387816e-01, -1.4217815e-01,  2.4479778e+00],\n",
      "       [-1.9435003e+00, -2.2985373e+00, -1.4532737e+00, ...,\n",
      "        -3.0432636e-01, -5.8264035e-01, -7.8789055e-01],\n",
      "       [ 1.1651335e+00,  1.0641365e+00,  8.1982738e-01, ...,\n",
      "         3.6481941e-01, -2.1518171e-01,  1.9613144e+00],\n",
      "       ...,\n",
      "       [-6.3597476e-01, -7.6162183e-01, -4.2745855e-01, ...,\n",
      "        -1.2282431e-03, -2.8147373e-01, -3.8136342e-01],\n",
      "       [ 1.4871033e+00,  1.7776088e+00,  1.1610185e+00, ...,\n",
      "         3.3688587e-01, -1.2941676e-01,  2.6795111e+00],\n",
      "       [ 8.2748657e-01,  1.3006197e+00,  1.0163417e+00, ...,\n",
      "         2.2275090e-01, -3.2809320e-01,  1.5615406e+00]], dtype=float32), h=array([[ 4.9121070e-01,  5.4205418e-01,  4.5592764e-01, ...,\n",
      "         8.2614839e-02, -5.2119788e-02,  7.5862122e-01],\n",
      "       [-7.6771355e-01, -7.5568753e-01, -6.3613087e-01, ...,\n",
      "        -1.9256996e-01, -2.8309715e-01, -2.2635114e-01],\n",
      "       [ 3.9881143e-01,  3.7882546e-01,  3.5823014e-01, ...,\n",
      "         1.3479094e-01, -1.0105119e-01,  7.0673525e-01],\n",
      "       ...,\n",
      "       [-3.5749987e-01, -4.2002776e-01, -2.6283965e-01, ...,\n",
      "        -5.3576939e-04, -1.6084677e-01, -1.6817209e-01],\n",
      "       [ 5.0387490e-01,  4.1661021e-01,  4.1066346e-01, ...,\n",
      "         1.0046204e-01, -4.9907796e-02,  8.1918943e-01],\n",
      "       [ 4.0851593e-01,  5.8997941e-01,  5.0221574e-01, ...,\n",
      "         8.3299428e-02, -1.6958804e-01,  7.2919524e-01]], dtype=float32)), LSTMStateTuple(c=array([[ 3.4857111e+00, -3.1004490e-03,  2.6804099e+00, ...,\n",
      "         3.2084706e+00, -2.3629403e-01, -4.0299630e+00],\n",
      "       [-2.3385129e+00,  3.9995565e+00, -5.2592158e-01, ...,\n",
      "        -1.5899970e-01,  3.8835208e+00,  1.6431103e+00],\n",
      "       [ 2.6159723e+00, -4.1615176e-03,  2.0048099e+00, ...,\n",
      "         2.3931715e+00, -2.9690591e-01, -3.0260055e+00],\n",
      "       ...,\n",
      "       [-7.5973618e-01,  1.1327627e+00, -4.1872114e-01, ...,\n",
      "        -2.6731500e-01,  1.1253107e+00,  6.7826664e-01],\n",
      "       [ 3.4957395e+00, -2.7487115e-03,  2.6750135e+00, ...,\n",
      "         3.2046916e+00, -2.3026279e-01, -4.0063329e+00],\n",
      "       [ 1.9128888e+00, -1.1184757e-02,  1.5529531e+00, ...,\n",
      "         1.7025169e+00, -3.7422729e-01, -2.2349801e+00]], dtype=float32), h=array([[ 9.6539998e-01, -3.5430694e-05,  8.5538816e-01, ...,\n",
      "         9.8346752e-01, -1.7011721e-02, -9.4269300e-01],\n",
      "       [-8.6750269e-01,  9.9781215e-01, -8.9685306e-02, ...,\n",
      "        -3.4708236e-03,  9.9203527e-01,  4.6636921e-01],\n",
      "       [ 9.4849581e-01, -6.5174994e-05,  8.1386834e-01, ...,\n",
      "         9.6506935e-01, -2.6410110e-02, -9.2113996e-01],\n",
      "       ...,\n",
      "       [-4.2871478e-01,  7.4985892e-01, -1.2770745e-01, ...,\n",
      "        -4.6081688e-02,  6.9801098e-01,  3.1987444e-01],\n",
      "       [ 9.6158350e-01, -3.2193988e-05,  8.5678256e-01, ...,\n",
      "         9.8286706e-01, -1.6690508e-02, -9.4167775e-01],\n",
      "       [ 8.9246649e-01, -3.3381814e-04,  7.5175279e-01, ...,\n",
      "         9.0320230e-01, -4.8433334e-02, -8.7531930e-01]], dtype=float32)))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(1000):\n",
    "        x_batch, y_batch, seqlen_batch = get_sent_batch(batch_size, train_x, train_y, train_seq_lens)\n",
    "        sess.run(train_op, feed_dict={X: x_batch, Y: y_batch, _seqlens: seqlen_batch})\n",
    "        \n",
    "        if not epoch % 100:\n",
    "            _loss, acc = sess.run([loss, acc_op], feed_dict={X: x_batch, Y: y_batch, _seqlens: seqlen_batch})\n",
    "            print('Epoch: ', epoch, ' Loss: ', _loss, ' Acc: ', acc)\n",
    "    \n",
    "    for i in range(5):\n",
    "        x_test, y_test, seqlen_test = get_sent_batch(batch_size, test_x, test_y, test_seq_lens)\n",
    "        pred, test_acc = sess.run([tf.argmax(logits, 1), acc_op], feed_dict={X: x_test, Y: y_test, _seqlens: seqlen_test})\n",
    "        print('Acc: ', acc)\n",
    "    \n",
    "    output_ex, state_ex = sess.run([outputs, states], feed_dict={X: x_test, Y: y_test, _seqlens: seqlen_test})\n",
    "    print('output: ', output_ex)\n",
    "    print('state: ', state_ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
